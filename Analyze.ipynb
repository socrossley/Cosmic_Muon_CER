{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import gc\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data location: ./data/simulated_cosmics.root\n"
     ]
    }
   ],
   "source": [
    "full = False\n",
    "\n",
    "if full:\n",
    "    data_loc = r\"./data/simulated_cosmics_full.root\"\n",
    "else:\n",
    "    data_loc = r\"./data/simulated_cosmics.root\"\n",
    "print(\"Using data location:\", data_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't import ROOT unless absolutely necessary (takes a long time)\n",
    "# import ROOT\n",
    "import uproot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<TTree 'CalorimetryAnalyzer' (101 branches) at 0x7fbfbc4c1080>,\n",
       " <TTree 'CalorimetryAnalyzer' (101 branches) at 0x7fbfbc4c13c8>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = uproot.open(data_loc)\n",
    "file.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 3 for full dataset, 0 for smaller dataset\n",
    "idx = 0\n",
    "tree = file.values()[idx]\n",
    "per_particle_variables = ['backtracked_e','backtracked_pdg','backtracked_purity']\n",
    "variables = ['dedx_y','rr_y','pitch_y']\n",
    "slimmer_variables = ['trk_sce_start_x','trk_sce_start_y','trk_sce_start_z', 'trk_sce_end_x','trk_sce_end_y','trk_sce_end_z','backtracked_e', 'backtracked_pdg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag for removal\n",
    "The following cell populates the list <code>idxs_to_remove</code>, tagging the relevant rows of the dataframe for removal. For now, removal criterion is based solely on whether we think the particle both enters and exits the detector. If it neither enters nor exits at a boundary, particle is tagged for removal.\n",
    "- TODO: Speed this up using jit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate principal dataframe and slim\n",
    "In the future, we will want to do this in batches, as even the slimmed data will be too large to hold in memory all at once. Here, the data is loaded to memory in its entirety, and then slimmed accordingly. Even if we slim better, there is no way around loading the data entirely first before slimming (at least, not that I know of, uproot documentation seems to suggest no - there may be a way in  raw C++)\n",
    "- (IMPLEMENTED) It may be better to make two dataframes, one containing the elements that are always the same for a given particle (<code>backtracked_e</code> etc.) and one containing the data points (<code>dedx_y</code>). This should take up less memory as the current implementation of uproot handles these two types of data in the same dataframe by copying the value of backtracked_e for each of the data points in dedx_y, using up a lot more memory than necessary (I think, even if they are just filled with pointers to the same memory address)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Slimming Mask...\n",
      "Will remove 1121 particles\n"
     ]
    }
   ],
   "source": [
    "def distance_to_edge(r):\n",
    "    dimensions = np.array([[0, 256], [-116,116], [0,1036]])\n",
    "    return  np.min(np.abs(dimensions - r[:, np.newaxis]))\n",
    "\n",
    "print(\"Preparing Slimming Mask...\")\n",
    "slimmerdf = tree.arrays(slimmer_variables, library='pd')\n",
    "\n",
    "start_dists, end_dists = np.array([ [distance_to_edge(r[:3]), distance_to_edge(r[3:6])] for _, r in slimmerdf.iterrows() ]).T\n",
    "energy_mask = (slimmerdf.backtracked_e > 0) & (slimmerdf.backtracked_e < 300) & (np.abs(slimmerdf.backtracked_pdg) == 13)\n",
    "mask = ((start_dists < 2) & (end_dists < 2) & energy_mask).to_numpy()\n",
    "print(\"Will remove\", np.sum(~mask), \"particles\")\n",
    "mask = mask[df.index.get_level_values(0)] # Broadcast to multiindex shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Principal Dataframe...\n",
      "Loaded dedx_y data...\n",
      "[ True  True  True ...  True  True  True]\n",
      "Loaded rr_y data...\n",
      "Loaded pitch_y data...\n",
      "Generated!\n",
      "Original Size: 45.395421 MB\n",
      "Slimmed Size: 26.465695 MB\n"
     ]
    }
   ],
   "source": [
    "# Generate DataFrame with the data\n",
    "# There seems to be a memory leak in pandas https://github.com/pandas-dev/pandas/issues/2659. This casues the \n",
    "# allocated memory for the dataframe to be much higher than required. As of now there is no simple fix that I \n",
    "# can find, so I will have to work around it.\n",
    "# Maybe look into this further later if it is a problem with the larger dataset.\n",
    "\n",
    "print(\"Generating Principal Dataframe...\")\n",
    "part_df = tree.arrays(per_particle_variables, library='pd')\n",
    "df = tree.arrays(variables[0], library='pd')\n",
    "print(\"Loaded\", variables[0], \"data...\")\n",
    "size = sys.getsizeof(df)\n",
    "\n",
    "# Slim according to mask\n",
    "df = df.loc[mask, :]\n",
    "\n",
    "# This loop loads in the next column of the dataframe, slims it, and appends it to df\n",
    "for name in variables[1:]:\n",
    "    next_col = tree.arrays(name, library='pd')\n",
    "    print(\"Loaded\", name, \"data...\")\n",
    "    size += sys.getsizeof(next_col[name])\n",
    "    next_col = next_col.loc[mask, :]\n",
    "    df = df.join(next_col, on=['entry', 'subentry'])\n",
    "\n",
    "print(\"Generated!\")\n",
    "print(\"Original Size:\", size/10**6, \"MB\")\n",
    "print(\"Slimmed Size:\", sys.getsizeof(df)/10**6, \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin Analysis\n",
    "The following cell initializes all the necessary variables to be used in the analysis loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize some debug counting variables\n",
    "These variables keep track of some important data regarding how many particles are ignored, how many data points are ignored, and the number of data points / particles that are ignored for each of the various possible reasons. This way we can keep track of the main reasons why data from certain particles is not being considered.\n",
    "- Move info on nege, highe, non-muon, and the number of bad particles to slimming section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_whole_particle_debug_data(e, pur, mu):\n",
    "    global num_impurities, num_mu, num_amu, num_notmu\n",
    "    \n",
    "    # Ideally we don't use this metric for a selection cut\n",
    "    if pur <= 0.9:\n",
    "        num_impurities += 1\n",
    "\n",
    "    if mu == 13:\n",
    "        num_mu += 1\n",
    "    elif mu == -13:\n",
    "        num_amu += 1\n",
    "    else:\n",
    "        print('Error: non-muon detected')\n",
    "        num_notmu += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datapoint_is_invalid(de, dedx, lovercostheta, e, dedx_cutoff, pitch_high_cutoff):\n",
    "    global num_highde, num_highdedx, num_highpitch\n",
    "    skip_rest = False\n",
    "    \n",
    "    # TODO: cut drastic changes in pitch\n",
    "    if de > e:\n",
    "        skip_rest = True\n",
    "        num_highde += 1\n",
    "    if dedx > dedx_cutoff:\n",
    "        skip_rest = True\n",
    "        num_highdedx += 1\n",
    "    if lovercostheta > pitch_high_cutoff:\n",
    "        skip_rest = True\n",
    "        num_highpitch += 1\n",
    "        \n",
    "    return skip_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_debug_data():\n",
    "    global num_impurities, num_mu, num_amu, num_notmu, num_highde, num_highdedx, num_highpitch, num_particles_partially_skipped, num_skipped_dp\n",
    "    \n",
    "    print(\"Impurities:\", num_impurities)\n",
    "    print(\"Muons:\", num_mu)\n",
    "    print(\"Antimuons:\", num_amu)\n",
    "    print(\"Particles partially skipped:\", num_particles_partially_skipped)\n",
    "    print(\"Total skipped data points:\", num_skipped_dp)\n",
    "    print(\"Particles with a high dE data point:\", num_highde)\n",
    "    print(\"Particles with a high dEdx data point:\", num_highdedx)\n",
    "    print(\"Particles with a high pitch data point:\", num_highpitch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_all_counts():\n",
    "    global num_impurities, num_mu, num_amu, num_notmu, num_highde, num_highdedx, num_highpitch, num_particles_partially_skipped, num_skipped_dp, p_count\n",
    "    \n",
    "    num_impurities = 0\n",
    "    num_mu = 0\n",
    "    num_amu = 0\n",
    "    num_particles_partially_skipped = 0\n",
    "    p_count = 0\n",
    "    num_skipped_dp = 0\n",
    "    num_highde = 0\n",
    "    num_highdedx = 0\n",
    "    num_highpitch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Principal Analysis Loop\n",
    "- TODO: Change how debug counts are handled. Use a dictionary instead and pass that into the relevant functions, rather than the clunky use of global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------Initialize debug counting variables-----------------------------------------\n",
    "# particle counts\n",
    "num_impurities = 0\n",
    "num_mu = 0\n",
    "num_amu = 0\n",
    "num_particles_partially_skipped = 0\n",
    "p_count = 0\n",
    "\n",
    "# data point counts\n",
    "num_skipped_dp = 0\n",
    "num_highde = 0\n",
    "num_highdedx = 0\n",
    "num_highpitch = 0\n",
    "    \n",
    "def analyze_data(df):\n",
    "    reset_all_counts()\n",
    "    global p_count, num_particles_partially_skipped, num_skipped_dp\n",
    "    \n",
    "    #--------------------------------------Instantiate principal data arrays---------------------------------------\n",
    "    dedxs = []\n",
    "    es = []\n",
    "    pitches = []\n",
    "\n",
    "    #-----------------------------------------Initialize relevant raw data-----------------------------------------\n",
    "    e_losses_per_step = df['dedx_y']\n",
    "    true_es = part_df['backtracked_e']\n",
    "    rrange = df['rr_y']\n",
    "    pitch = df['pitch_y']\n",
    "    purity = part_df['backtracked_purity']\n",
    "    mu_type = part_df['backtracked_pdg']\n",
    "    particle_idxs = df.index.get_level_values(0).unique()\n",
    "\n",
    "    #-------------------------------------Initialize selection cut variables---------------------------------------\n",
    "    dedx_cutoff = 100\n",
    "    pitch_high_cutoff = 0.3 / np.cos(70*np.pi/180)     # Multiplied by 3mm for wire spacing\n",
    "    # pitch of greater than 70 degrees wrt collection plane is ignored\n",
    "    \n",
    "    #---------------------------------------Initialize analysis time counts----------------------------------------\n",
    "    tot_particles = len(particle_idxs)\n",
    "    pcnt_per_count = 100./tot_particles\n",
    "    count_per_pcnt = 1/pcnt_per_count\n",
    "    running_count_for_pcnt_increment = 0\n",
    "\n",
    "    #-----------------------------------------------Start loop-----------------------------------------------------\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    print(\"Analyzing...\")\n",
    "    for p in particle_idxs:\n",
    "\n",
    "        if p_count > running_count_for_pcnt_increment:\n",
    "            print(f\"{(running_count_for_pcnt_increment / tot_particles)*100:.0f}%        \", end = '\\r', flush=True)\n",
    "            running_count_for_pcnt_increment += count_per_pcnt\n",
    "\n",
    "        p_count += 1\n",
    "        data_points = df.loc[p,:].index\n",
    "        prev_range = 0\n",
    "\n",
    "        e = true_es[p]                               # True energy of the particle (GeV)\n",
    "        pur = purity[p]\n",
    "        mu = mu_type[p]\n",
    "\n",
    "        update_whole_particle_debug_data(e, pur, mu)        \n",
    "\n",
    "        for d in data_points:\n",
    "            i = (p,d)\n",
    "            x = rrange[i]                                # Particle current x\n",
    "            dedx = e_losses_per_step[i]                  # Particle recent energy loss (MeV/cm)\n",
    "            lovercostheta = pitch[i]                         # Pitch (For collection wires spaced by 3mm)\n",
    "            de = (rrange[i] - prev_range)*dedx/1000      # Approx energy lost since last step (GeV)\n",
    "\n",
    "            if datapoint_is_invalid(de, dedx, lovercostheta, e, dedx_cutoff, pitch_high_cutoff):\n",
    "                num_particles_partially_skipped += 1\n",
    "                num_skipped_dp += len(data_points) - d\n",
    "                break\n",
    "            else:\n",
    "                es.append(e)\n",
    "                dedxs.append(dedx)\n",
    "                pitches.append(lovercostheta)\n",
    "                e -= de                                  # Lower energy accordingly\n",
    "                prev_range = rrange[i]                   # Update prev_range\n",
    "\n",
    "\n",
    "    print(\"100%     \")\n",
    "    end = time.perf_counter()\n",
    "    t = end-start\n",
    "    print(f'Done! Analysis time: {int(t//60)}m {t%60:0.1f}s')  \n",
    "    print_debug_data()\n",
    "    \n",
    "    return es, dedxs, pitches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "100%       \n",
      "Done! Analysis time: 1m 8.7s\n",
      "Impurities: 1492\n",
      "Muons: 2370\n",
      "Antimuons: 2431\n",
      "Particles partially skipped: 2979\n",
      "Total skipped data points: 397859\n",
      "Particles with a high dE data point: 3\n",
      "Particles with a high dEdx data point: 145\n",
      "Particles with a high pitch data point: 2836\n"
     ]
    }
   ],
   "source": [
    "es, dedxs, pitches = analyze_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(name):\n",
    "    path = r'./data/' + name\n",
    "    i = input(f\"Are you sure you want to overwrite {path}?\")\n",
    "    if i == \"yes\":\n",
    "        with open(r'./data/' + name, 'w', newline='') as save:\n",
    "            writer = csv.writer(save)\n",
    "            writer.writerow(es)\n",
    "            writer.writerow(dedxs)\n",
    "            writer.writerow(pitches)\n",
    "        print(f\"saved to {path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if full:\n",
    "    save_file('Analyzed_Data_Full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
